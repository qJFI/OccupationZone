\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{enumitem}
\usepackage{booktabs}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}

\begin{titlepage}
\begin{center}
\vspace*{2cm}

{\huge\bfseries WorkZone: Job Application\\[0.5cm]Management System\\[2cm]}

{\Large Technical Documentation\\[2cm]}

{\large Information Retrieval Project\\[1cm]}

{\large May 22, 2025}

\vfill
\end{center}
\end{titlepage}

\newpage
\tableofcontents
\newpage

\section{Introduction}

\subsection{Project Overview}
WorkZone is a comprehensive job application management system designed to streamline the job search and application process. It integrates web scraping, data management, and application automation into a cohesive platform. This document details the system's architecture, implementation choices, and core functionality.

\subsection{Motivation}
The job application process often involves:
\begin{itemize}
    \item Searching across multiple job platforms
    \item Manually tracking applications
    \item Repeatedly entering the same information
    \item Managing resume versions
\end{itemize}

We developed WorkZone to address these pain points by automating and centralizing the entire process, providing a single interface for managing all job search activities.

\section{System Architecture}

\subsection{Component Overview}
The system is built around five core components:

\begin{tcolorbox}[colback=backcolour,title=System Components]
\begin{enumerate}
    \item \textbf{Web Scrapers}: Collection of specialized scrapers for different job platforms
    \item \textbf{Data Storage}: SQLite database with job and resume information
    \item \textbf{Automation Engine}: Platform-specific application submission automation
    \item \textbf{Web Interface}: Streamlit-based user interface
    \item \textbf{Utilities}: Support functions for logging and data management
\end{enumerate}
\end{tcolorbox}

\subsection{Data Flow}
\begin{enumerate}
    \item Web scrapers collect job listings from multiple sources
    \item Jobs are stored in a centralized SQLite database
    \item Users search and filter jobs via the Streamlit interface
    \item Users upload and manage resumes via the interface
    \item The automation engine applies to selected jobs with user credentials
\end{enumerate}

\section{Implementation Details}

\subsection{Web Scrapers}
We implemented specialized scrapers for seven different job platforms, each tailored to the specific HTML structure and data patterns of its target site.

\subsubsection{Design Choices}
We chose to implement individual scraper classes for each platform to maximize flexibility and maintainability:

\begin{lstlisting}[language=Python, caption=Scraper Class Structure]
# From crawler.py
scraper_classes = [
    LinkedInScraper,
    FreelancerScraper,
    WuzzufScraper,
    RemoteOKScraper,
    WeWorkRemotelyScraper,
    UpworkScraper,
    PeoplePerHourScraper
]
\end{lstlisting}

Each scraper implements a common interface with a \texttt{scrape()} method, allowing for consistent integration with the main crawler.

\subsubsection{Concurrent Execution}
To optimize data collection, we implemented concurrent scraping using Python's \texttt{ThreadPoolExecutor}:

\begin{lstlisting}[language=Python, caption=Concurrent Scraping Implementation]
# From crawler.py
with concurrent.futures.ThreadPoolExecutor(max_workers=len(scraper_classes)) as executor:
    futures = {executor.submit(run_scraper, scraper_class, query, db_name): scraper_class.__name__ 
               for scraper_class in scraper_classes}
    for future in concurrent.futures.as_completed(futures):
        scraper_name = futures[future]
        try:
            future.result()
            logging.info(f"{scraper_name} completed successfully")
        except Exception as e:
            logging.error(f"{scraper_name} generated an exception: {e}")
\end{lstlisting}

This approach offers several advantages:
\begin{itemize}
    \item Parallel execution reduces total scraping time
    \item Each scraper runs independently with its own database connection
    \item Error handling isolates failures to individual scrapers
\end{itemize}

\subsubsection{Platform-Specific Implementations}
Each scraper is tailored to its target platform. For example, the LinkedIn scraper handles authentication, pagination, and job extraction specific to LinkedIn's structure:

\begin{lstlisting}[language=Python, caption=LinkedIn Scraper (Excerpt)]
# From linkedin.py
class LinkedInScraper:
    def __init__(self, storage, query="software engineer"):
        self.storage = storage
        self.query = query
        self.driver = webdriver.Chrome()
        
    def scrape(self, max_pages=5):
        # LinkedIn-specific implementation
        # ...
\end{lstlisting}

\subsection{Data Management}

\subsubsection{Job Model}
We created a standardized \texttt{Job} class to normalize data across platforms:

\begin{lstlisting}[language=Python, caption=Job Data Model]
# From models.py
class Job:
    def __init__(self, title=None, description=None, link=None, company=None, 
                 source=None, timestamp=None, location=None):
        self.id = str(uuid.uuid4())
        self.title = title or 'non'
        self.description = description or 'non'
        self.link = link or 'non'
        self.company = company or 'non'
        self.source = source or 'non'
        self.timestamp = timestamp or datetime.now().isoformat()
        self.location = location or 'non'
\end{lstlisting}

Key design decisions:
\begin{itemize}
    \item UUID-based primary key for database integrity
    \item Default values to ensure data consistency
    \item ISO format timestamp for standardization
    \item Common fields across all job platforms
\end{itemize}

\subsubsection{Data Storage}
We chose SQLite for data persistence due to its simplicity, portability, and performance:

\begin{lstlisting}[language=Python, caption=DataStorage Implementation]
# From models.py
class DataStorage:
    def __init__(self, output_format='sqlite', db_name='jobs.db'):
        self.jobs = []
        self.output_format = output_format
        self.db_name = db_name
        self.conn = sqlite3.connect(self.db_name)
        self.create_table()
\end{lstlisting}

The \texttt{DataStorage} class provides an abstraction layer over the database operations:
\begin{itemize}
    \item Table creation and schema management
    \item Job insertion with duplicate handling
    \item Connection management
    \item Support for multiple output formats (CSV, JSON)
\end{itemize}

\subsubsection{Deduplication}
To prevent duplicate job listings, we implemented a deduplication utility:

\begin{lstlisting}[language=Python, caption=Deduplication Implementation]
# From utils.py
def deduplicate_jobs(conn):
    with conn:
        conn.execute("""
            DELETE FROM jobs
            WHERE rowid NOT IN (
                SELECT MIN(rowid)
                FROM jobs
                GROUP BY link
            )
        """)
\end{lstlisting}

This SQL query efficiently removes duplicates while keeping the earliest record (by rowid) for each unique job link.

\subsection{Automation Engine}

\subsubsection{Platform-Specific Appliers}
We developed automation modules for three major platforms:
\begin{itemize}
    \item LinkedIn
    \item Wuzzuf
    \item Freelancer
\end{itemize}

Each automation module handles the specific workflow of its target platform:

\begin{lstlisting}[language=Python, caption=Job Application Automation]
# From job_applier.py
def apply_to_job(job, resume_path, labels, credentials):
    source = job['source'].lower()
    if source == 'linkedin':
        return apply_to_linkedin_job(job['link'], resume_path, labels, 
                                      credentials['linkedin_email'], 
                                      credentials['linkedin_password'])
    elif source == 'wuzzuf':
        return apply_to_wuzzuf_job(job['link'], resume_path, labels, 
                                    credentials['wuzzuf_email'], 
                                    credentials['wuzzuf_password'])
    # ...
\end{lstlisting}

\subsubsection{Automation Techniques}
The automation modules use Selenium WebDriver to interact with job platforms:
\begin{itemize}
    \item Automated form filling
    \item Resume upload
    \item Authentication handling
    \item Application status tracking
\end{itemize}

\begin{lstlisting}[language=Python, caption=LinkedIn Application Automation (Excerpt)]
# From linkedin_applier.py
def apply_to_linkedin_job(job_url, resume_path, labels, email, password):
    driver = webdriver.Chrome()
    try:
        # Login to LinkedIn
        driver.get("https://www.linkedin.com/login")
        # ... authentication logic ...
        
        # Navigate to job
        driver.get(job_url)
        # ... application logic ...
        
        return True, "Application submitted successfully."
    except Exception as e:
        return False, f"Failed to apply: {str(e)}"
    finally:
        driver.quit()
\end{lstlisting}

\subsection{User Interface}

\subsubsection{Framework Selection}
We chose Streamlit for the user interface due to its:
\begin{itemize}
    \item Rapid development capabilities
    \item Interactive widgets
    \item Python integration
    \item Minimal frontend code requirements
\end{itemize}

\subsubsection{Main Application Structure}
The main application provides navigation and authentication:

\begin{lstlisting}[language=Python, caption=Main Streamlit Application]
# From main.py
import streamlit as st
from streamlit_option_menu import option_menu
from auth import login, logout, get_current_user

st.set_page_config(page_title="Job Application Manager")

# User authentication
user = get_current_user()
if not user:
    login()
    st.stop()

# Navigation menu
selected = option_menu(
    "Main Menu",
    ["Job Search", "Saved Jobs", "Applications", "Resume Manager", 
     "Analytics", "Settings"],
    icons=["search", "bookmark", "check2-circle", "file-earmark-person", 
           "bar-chart", "gear"],
    menu_icon="cast",
    default_index=0,
    orientation="horizontal"
)

# Page routing
if selected == "Job Search":
    from my_pages.job_search import job_search_page
    job_search_page()
# ... other pages ...
\end{lstlisting}

\subsubsection{Advanced Search Implementation}
We implemented an advanced search interface with multiple filtering options:

\begin{lstlisting}[language=Python, caption=Advanced Search Implementation]
# From job_search.py
# Advanced Filters
with st.expander("Advanced Search Options", expanded=True):
    col1, col2, col3 = st.columns(3)
    with col1:
        title_filter = st.text_input("Job Title")
        # Multi-source selection
        all_sources = sorted(df["source"].unique().tolist())
        source_filter = st.multiselect("Source(s)", ["All"] + all_sources, 
                                         default=["All"])
    # ... other filters ...
    
    # Date range filter
    parsed_timestamps = pd.to_datetime(df["timestamp"], errors="coerce", 
                                       format='mixed')
    min_date = parsed_timestamps.min()
    max_date = parsed_timestamps.max()
    date_range = st.slider(
        "Date Range (Timestamp)",
        min_value=min_date,
        max_value=max_date,
        value=(min_date, max_date),
        format="YYYY-MM-DD"
    )
\end{lstlisting}

\subsubsection{Resume Management}
The resume manager page enables:
\begin{itemize}
    \item PDF/DOCX resume upload
    \item Resume preview
    \item Metadata management
    \item Information categorization
\end{itemize}

\begin{lstlisting}[language=Python, caption=Resume Manager Implementation (Excerpt)]
# From resume_manager.py
def resume_manager_page():
    st.header("Resume Manager")
    user = get_current_user()
    
    # ... database setup ...
    
    # Upload resume section
    uploaded_file = st.file_uploader("Upload or Update Resume", 
                                    type=["pdf", "docx"], 
                                    key=f"resume_upload_{user}")
    
    # ... resume processing ...
    
    # Display form for labels
    st.subheader("Resume Information")
    
    # Group labels by category
    categories = {
        "Personal Information": ["name", "age", "national_id", ...],
        "Professional Summary": ["overview", "career_objective"],
        # ... other categories ...
    }
    
    # Create tabs for different categories
    category_tabs = st.tabs(list(categories.keys()))
\end{lstlisting}

\section{Technical Challenges and Solutions}

\subsection{Challenge: Concurrent Database Access}
\textbf{Problem:} Multiple scrapers attempting to write to the same database simultaneously caused connection conflicts.

\textbf{Solution:} We implemented a separate database connection for each scraper thread:

\begin{lstlisting}[language=Python, caption=Independent Database Connections]
# From crawler.py
def run_scraper(scraper_class, query, db_name):
    storage = DataStorage(output_format='sqlite', db_name=db_name)
    scraper = scraper_class(storage, query=query)
    scraper.scrape(max_pages=5)
    del storage  # Ensure connection is closed
    return scraper_class.__name__
\end{lstlisting}

\subsection{Challenge: Duplicate Job Listings}
\textbf{Problem:} The same job might appear on multiple platforms or in multiple search results.

\textbf{Solution:} We implemented SQL-based deduplication using the job URL as the unique identifier:

\begin{lstlisting}[language=SQL, caption=Deduplication SQL]
DELETE FROM jobs
WHERE rowid NOT IN (
    SELECT MIN(rowid)
    FROM jobs
    GROUP BY link
)
\end{lstlisting}

\subsection{Challenge: Widget Key Conflicts in Streamlit}
\textbf{Problem:} Multiple identical widgets caused `DuplicateWidgetID` errors in the UI.

\textbf{Solution:} We implemented dynamic key generation for all widgets:

\begin{lstlisting}[language=Python, caption=Dynamic Widget Keys]
# Unique file uploader key
uploaded_file = st.file_uploader(
    "Upload or Update Resume", 
    type=["pdf", "docx"], 
    key=f"resume_upload_{user}"
)

# Unique button keys with UUID
unique_id = str(uuid.uuid4())
st.button("Apply Now", key=f"apply_{row['id']}_{idx}_{unique_id}")
\end{lstlisting}

\subsection{Challenge: Date Parsing in Timestamp Filtering}
\textbf{Problem:} Inconsistent timestamp formats caused parsing errors.

\textbf{Solution:} We implemented robust timestamp parsing:

\begin{lstlisting}[language=Python, caption=Robust Timestamp Parsing]
parsed_timestamps = pd.to_datetime(df["timestamp"], 
                                 errors="coerce", 
                                 format='mixed')
\end{lstlisting}

\section{Conclusion and Future Work}

\subsection{Achievements}
The WorkZone system successfully:
\begin{itemize}
    \item Integrates job data from seven major platforms
    \item Provides advanced search and filtering capabilities
    \item Manages resume uploads and metadata
    \item Automates application submission for three major platforms
    \item Centralizes job application tracking
\end{itemize}

\subsection{Future Enhancements}
Potential future developments include:
\begin{itemize}
    \item Machine learning for job matching and recommendations
    \item Additional platform integrations
    \item Enhanced resume parsing and optimization
    \item API development for mobile applications
    \item Interview scheduling and preparation tools
\end{itemize}

\subsection{Lessons Learned}
This project reinforced several key principles:
\begin{itemize}
    \item The importance of modular design for system extensibility
    \item Effective error handling for robust scraping
    \item Database optimization techniques for performance
    \item UI/UX considerations for complex data interaction
    \item The power of automation in streamlining repetitive tasks
\end{itemize}

\end{document}